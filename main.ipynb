{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8214db",
   "metadata": {},
   "source": [
    "This tutorial is based on the paper \n",
    "\"Communication-Efficient Learning of Deep Networks from Decentralized Data\"\n",
    "which introduced the concept of Federated Learning in 2017\n",
    "\n",
    "This papers introduces a new decentralized approach to Machine Learning in which many separate devices learn a local model from their local data, thus those chunks of local data never leaves the device. The local models are sent to a global server which aggregates those models to create a global model.\n",
    "\n",
    "This framework has introduced as a way of training the Google Keyboard preditive text feature across many Google (Android) smartphones that used the keyboard. The main driving force for the development of this framework was the possibility of training a global model without having to copy user data\n",
    "from every smartphone due to privacy concerns\n",
    "\n",
    "The federated learning experiment runs like this:\n",
    "\n",
    "1. A fraction of devices are selected from a population of devices\n",
    "2. Those devices receive the current \"global model\", which can be an uninitialized model\n",
    "3. Those devices run a certain number of local epochs using their data and this global model\n",
    "4. Each device will have its own local model which will be sent to the global server\n",
    "5. The global server will aggregate all those local models by means of a simple weighted averaging\n",
    "6. The updated global model will be distributed again to a new population of random devices (back to step 2)\n",
    "\n",
    "\n",
    "The above loop will run for a certain number of global epochs, and then the training will be done and a final global model obtained.\n",
    "\n",
    "The code in this notebook will demonstrate how this process works using convolutional neural networks and fully connected networks with the MNIST, FMNIST and CIFAR datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ec02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, importing all the necessary Python modules\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This module is where the actual local learning of each device will happen\n",
    "from update import LocalUpdate, test_inference\n",
    "\n",
    "# This module is defining the CNN's and FC networks that will be used in the tutorial\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "\n",
    "# Those are utility functions to get the data, average model weights, etc\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "\n",
    "# To fix potential issues with matplot lib\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class contains all the parameter for the experiment\n",
    "# which are explained below\n",
    "class Args:\n",
    "    \n",
    "  # This sets the number of global epochs the experiment will run\n",
    "  global_epochs = 10\n",
    "\n",
    "  # This sets the size of the population (how many local devices may participate in the experiment) \n",
    "  num_users = 100\n",
    "\n",
    "  # This is the fraction of devices that are selected to participate in the experiment\n",
    "  # every global epoch    \n",
    "  frac = 0.1\n",
    "\n",
    "  # This sets how many local epochs each device will run before sending its local model\n",
    "  # to the global server\n",
    "  local_ep = 5\n",
    "    \n",
    "  # This sets the batch size each device will use in its local training epochs\n",
    "  local_bs = 4\n",
    "\n",
    "  # This is the learning rate used by the devices\n",
    "  lr = 0.1\n",
    "\n",
    "  # This is the optimizer used by the devices\n",
    "  # It can be sgd or adam \n",
    "  optimizer = \"sgd\"  \n",
    "    \n",
    "  # This sets which model will be used\n",
    "  # it can be \"mlp\" (fully connected) or \"cnn\" (convolution)\n",
    "  model = \"cnn\"\n",
    "\n",
    "  # This sets how many channels the input data has\n",
    "  # All datasets in the experiment have 1 channel\n",
    "  num_channels = 1\n",
    "    \n",
    "  # Selects the dataset that will be used\n",
    "  # it can be: cifar, mnist or fmnist\n",
    "  dataset = \"cifar\"\n",
    "    \n",
    "  # All the datasets in the experiment have 10 classes  \n",
    "  num_classes = 10\n",
    "\n",
    "  # This sets if the experiment will use non-IID data\n",
    "  # non-IID data means that the data present on any given device\n",
    "  # is based on the device particular usage, hence its local dataset\n",
    "  # is not representative of the whole population distribution \n",
    "  iid = 1\n",
    "\n",
    "  # Set this to one to make every device selected for training\n",
    "  # have an unequal (random) amount of data\n",
    "  unequal = 1\n",
    "    \n",
    "  # If we want verbose logs, set this one. If not, set to zero\n",
    "  verbose = 1\n",
    "\n",
    "  # Value to set the Pytorch seed, to ensure reproducibility between runs   \n",
    "  seed = 1\n",
    "\n",
    "  # If the experiment will use the GPU for training   \n",
    "  gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Parses the experiments parameters\n",
    "args = Args()\n",
    "exp_details(args)\n",
    "\n",
    "# This variable is used by Pytorch to device if it is going to use GPU or not\n",
    "if args.gpu is None:\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "# Load select dataset and device population\n",
    "# Device population is the population which the devices selected for training\n",
    "# will be sampled from, each device will have its own local data\n",
    "train_dataset, test_dataset, device_population = get_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the selected model\n",
    "if args.model == 'cnn':\n",
    "    # Convolutional neural netork\n",
    "    if args.dataset == 'mnist':\n",
    "        global_model = CNNMnist(args=args)\n",
    "    elif args.dataset == 'fmnist':\n",
    "        global_model = CNNFashion_Mnist(args=args)\n",
    "    elif args.dataset == 'cifar':\n",
    "        global_model = CNNCifar(args=args)\n",
    "\n",
    "elif args.model == 'mlp':\n",
    "    # Multi-layer preceptron (fully connected network)\n",
    "    img_size = train_dataset[0][0].shape\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                           dim_out=args.num_classes)\n",
    "\n",
    "# Prints the model architecture on the screen\n",
    "print(global_model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0caf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to device (GPU or CPU).\n",
    "global_model.to(device)\n",
    "\n",
    "# Defining some helper variables for training\n",
    "train_loss, train_accuracy = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the experiment for the selected number of global epochs (rounds)\n",
    "for epoch in range(args.global_epochs):\n",
    "    \n",
    "    print(f'Global Training Round : {epoch+1}\\n')\n",
    "    \n",
    "    # Sample a fraction of the population\n",
    "    # Each device will be represented by an integer number (ID) in the\n",
    "    # \"selected_devices\" list     \n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    selected_devices = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    print(\"Devices selected in global round {}: {}\".format(epoch+1,selected_devices))\n",
    "\n",
    "    # Sets the model to train\n",
    "    global_model.train()\n",
    "\n",
    "     # Local weights and local losses are lists that keeps track of\n",
    "    # each selected device model weights and training loss    \n",
    "    local_weights, local_losses = [], []\n",
    "    # For each selected device, runs the local training\n",
    "    for device in selected_devices:\n",
    "        \n",
    "        # Creates an object of the class \"LocalUpdate\" which will represent\n",
    "        # each device selected for training\n",
    "        local_device = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=device_population[device], device_id=device)\n",
    "\n",
    "        # This function will run the device local training and return the\n",
    "        # obtained local model. Note that we are passing a full copy of the\n",
    "        # global model to the device\n",
    "        device_weights, device_loss = local_device.update_weights(\n",
    "            model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        \n",
    "        local_weights.append(copy.deepcopy(device_weights))\n",
    "        local_losses.append(copy.deepcopy(device_loss))\n",
    "        \n",
    "    # This function will take all local models weights and do a simple averaging\n",
    "    # of all those weights to obtain the new global model weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # Load the obtained weights into the global model structure\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    # Calculates the average loss of this global epoch by averaging all the local losses of every\n",
    "    # selected device     \n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    \n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every global epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    for device in range(args.num_users):\n",
    "        \n",
    "        # Creates a class to represent each the local device in the whole population\n",
    "        local_device = LocalUpdate(args=args, dataset=train_dataset,\n",
    "                                  idxs=device_population[device], device_id=device)\n",
    "\n",
    "        # Calculates the device accuracy and loss in its local data using the global model\n",
    "        acc, loss = local_device.inference(model=global_model)\n",
    "\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "    \n",
    "    print(f'\\nAvg Training Stats after {epoch+1} global epochs:')\n",
    "    print('Global Training Loss: {:.3f}'.format(np.mean(np.array(train_loss))))\n",
    "    print('Global Train Accuracy: {:.2f}%\\n'.format(100*train_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final global model after completion of training\n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "print(f' \\n Results after {args.global_epochs} global epochs of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "print('\\n Total Run Time: {0:0.4f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.title('Training Loss vs Communication rounds (global epochs)')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.show()\n",
    "plt.savefig('save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "            format(args.dataset, args.model, args.global_epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Average Accuracy vs Global Epochs\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Global Epochs')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='r')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "            format(args.dataset, args.model, args.global_epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24372eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
